import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from transformers import DebertaV2Tokenizer, get_linear_schedule_with_warmup
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix, classification_report
from collections import Counter
from sklearn.model_selection import StratifiedGroupKFold
from tqdm import tqdm
import swanlab
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import label_binarize
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥å¢å¼ºç‰ˆæ¨¡å—
from enhanced_data_deal import EnhancedSuicideDataset, SuicideDataset
from enhanced_model import EnhancedSuicideRiskClassifier, SuicideRiskClassifier
from enhanced_feature_utils import extract_enhanced_time_features, extract_temporal_patterns
# å¯¼å…¥CORALæ¨¡å—
from coral_model import CoralEnhancedSuicideRiskClassifier, CoralLoss, coral_predict
# å¯¼å…¥çº§è”åˆ†ç±»å™¨æ¨¡å—
from cascaded_model import CascadedSuicideRiskClassifier, CascadedLoss, cascaded_predict

# ç¯å¢ƒé…ç½®
# å¯é€‰ 0 1 2 3
os.environ["CUDA_VISIBLE_DEVICES"] = "1"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# torch.manual_seed(42)
# np.random.seed(42)
# 62
# 76
# 42

MODEL_NAME = '/home/zr/.cache/modelscope/hub/models/microsoft/deberta-v3-large'
save_path = '/home/zr/bigdata/card_model2/'
file_path = '/home/zr/bigdata/dataset/train.pkl'

def prepare_user_stratified_folds(df, n_splits=2):
    """
    æŒ‰ç”¨æˆ·åˆ†ç»„+æ ‡ç­¾åˆ†å±‚ï¼Œç”Ÿæˆäº¤å‰éªŒè¯æŠ˜
    """
    if 'user_id' not in df.columns:
        raise ValueError("æ•°æ®ä¸­ç¼ºå°‘ user_id åˆ—ï¼Œæ— æ³•æŒ‰ç”¨æˆ·åˆ’åˆ†ã€‚")
    if 'suicide_risk' not in df.columns:
        raise ValueError("æ•°æ®ä¸­ç¼ºå°‘ suicide_risk åˆ—ï¼Œæ— æ³•è¿›è¡Œåˆ†å±‚ã€‚")

    # ç»Ÿä¸€æ¯ä¸ªç”¨æˆ·çš„æ ‡ç­¾ï¼ˆå–ä¼—æ•°ï¼‰
    user_labels_map = {}
    for uid, group in df.groupby('user_id')['suicide_risk']:
        label_counts = Counter(group)
        main_label, count = label_counts.most_common(1)[0]
        if len(label_counts) > 1:
            print(f"âš ï¸ ç”¨æˆ· {uid} å­˜åœ¨å¤šæ ‡ç­¾ {dict(label_counts)}ï¼Œä½¿ç”¨å¤šæ•°æ ‡ç­¾ {main_label}")
        user_labels_map[uid] = main_label

    user_ids = list(user_labels_map.keys())
    labels = [user_labels_map[uid] for uid in user_ids]

    # ç”¨æˆ·çº§åˆ†å±‚äº¤å‰éªŒè¯
    sgkf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=42)
    folds = []
    for train_idx, val_idx in sgkf.split(user_ids, labels, groups=user_ids):
        train_ids = [user_ids[i] for i in train_idx]
        val_ids = [user_ids[i] for i in val_idx]
        train_df = df[df['user_id'].isin(train_ids)].reset_index(drop=True)
        val_df = df[df['user_id'].isin(val_ids)].reset_index(drop=True)
        folds.append((train_df, val_df))
    return folds

def train_epoch(model, dataloader, optimizer, criterion, scheduler, use_enhanced=True, use_coral=False, use_cascaded=False):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0
    
    for step, batch in enumerate(tqdm(dataloader, desc="Training")):
        try:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            time_features = batch['time_features'].to(device)
            labels = batch['label'].to(device)

            optimizer.zero_grad()
            
            if use_enhanced:
                # ä½¿ç”¨å¢å¼ºæ¨¡å‹
                pattern_features = batch['pattern_features'].to(device)
                outputs = model(input_ids, attention_mask, time_features, pattern_features)
            else:
                # ä½¿ç”¨åŸç‰ˆæ¨¡å‹
                outputs = model(input_ids, attention_mask, time_features)
            
            # æ ¹æ®ä½¿ç”¨çš„æ¨¡å‹ç±»å‹è°ƒæ•´æŸå¤±è®¡ç®—
            if use_coral:
                loss = criterion(outputs, labels)
            elif use_cascaded:
                stage1_logits, stage2_logits, stage3_logits = outputs
                loss = criterion(stage1_logits, stage2_logits, stage3_logits, labels)
            else:
                loss = criterion(outputs, labels)
                
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            scheduler.step()
            
            total_loss += loss.item()
        
            
            if step % 100 == 0:
                print(f"âœ… Step {step} å®Œæˆ, loss: {loss.item():.4f}")
        except Exception as e:
            print(f"âŒ Step {step} å‡ºé”™: {e}")
    
    return total_loss / len(dataloader)

def eval_epoch(model, criterion, dataloader, use_enhanced=True, use_coral=False, use_cascaded=False, num_classes=4):
    """éªŒè¯ä¸€ä¸ªepochï¼Œè¿”å›å¤šç§è¯„ä¼°æŒ‡æ ‡"""
    model.eval()
    preds, trues, probs = [], [], []
    total_loss = 0
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Validation"):
            try:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                time_features = batch['time_features'].to(device)
                labels = batch['label'].to(device)

                if use_enhanced:
                    pattern_features = batch['pattern_features'].to(device)
                    outputs = model(input_ids, attention_mask, time_features, pattern_features)
                else:
                    outputs = model(input_ids, attention_mask, time_features)
                
                # æ ¹æ®ä½¿ç”¨çš„æ¨¡å‹ç±»å‹è°ƒæ•´æŸå¤±è®¡ç®—å’Œé¢„æµ‹
                if use_coral:
                    loss = criterion(outputs, labels)
                    # ä½¿ç”¨CORALé¢„æµ‹å‡½æ•°
                    pred = coral_predict(outputs)
                elif use_cascaded:
                    stage1_logits, stage2_logits, stage3_logits = outputs
                    loss = criterion(stage1_logits, stage2_logits, stage3_logits, labels)
                    # ä½¿ç”¨çº§è”é¢„æµ‹å‡½æ•°
                    pred = cascaded_predict(stage1_logits, stage2_logits, stage3_logits)
                else:
                    loss = criterion(outputs, labels)
                    pred = outputs.argmax(dim=1)
                    
                total_loss += loss.item()
                
                preds.extend(pred.cpu().numpy())
                trues.extend(labels.cpu().numpy())
                
                # ä¿å­˜æ¦‚ç‡ç”¨äºè®¡ç®—AUC
                # å¯¹äºçº§è”åˆ†ç±»å™¨ï¼Œæˆ‘ä»¬éœ€è¦ç‰¹æ®Šå¤„ç†
                if use_cascaded:
                    # çº§è”åˆ†ç±»å™¨ä½¿ç”¨é¢„æµ‹å‡½æ•°è¿›è¡Œé¢„æµ‹ï¼Œä½†æˆ‘ä»¬ä»éœ€è¦æ¦‚ç‡æ¥è®¡ç®—AUC
                    # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨sigmoidæ¥è®¡ç®—æ¯ä¸ªé˜¶æ®µçš„æ¦‚ç‡
                    prob_stage1 = torch.sigmoid(stage1_logits).squeeze(-1)  # 0 vs (1,2,3)
                    prob_stage2 = torch.sigmoid(stage2_logits).squeeze(-1)  # 1 vs (2,3)
                    prob_stage3 = torch.sigmoid(stage3_logits).squeeze(-1)  # 2 vs 3
                    
                    # æ„é€ æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡
                    batch_size = len(pred)
                    batch_probs = np.zeros((batch_size, 4))
                    
                    # ç±»åˆ«0çš„æ¦‚ç‡ï¼šP(0) = 1 - P(>0) = 1 - prob_stage1
                    batch_probs[:, 0] = 1 - prob_stage1.cpu().numpy()
                    
                    # ç±»åˆ«1çš„æ¦‚ç‡ï¼šP(1) = P(>0) * (1 - P(>1)) = prob_stage1 * (1 - prob_stage2)
                    batch_probs[:, 1] = prob_stage1.cpu().numpy() * (1 - prob_stage2.cpu().numpy())
                    
                    # ç±»åˆ«2çš„æ¦‚ç‡ï¼šP(2) = P(>0) * P(>1) * (1 - P(>2)) = prob_stage1 * prob_stage2 * (1 - prob_stage3)
                    batch_probs[:, 2] = prob_stage1.cpu().numpy() * prob_stage2.cpu().numpy() * (1 - prob_stage3.cpu().numpy())
                    
                    # ç±»åˆ«3çš„æ¦‚ç‡ï¼šP(3) = P(>0) * P(>1) * P(>2) = prob_stage1 * prob_stage2 * prob_stage3
                    batch_probs[:, 3] = prob_stage1.cpu().numpy() * prob_stage2.cpu().numpy() * prob_stage3.cpu().numpy()
                    
                    probs.extend(batch_probs)
                else:
                    # æ™®é€šæ¨¡å‹å’ŒCORALæ¨¡å‹è¾“å‡ºå•ä¸ªå¼ é‡
                    prob = torch.softmax(outputs, dim=1)
                    probs.extend(prob.cpu().numpy())
                
            except Exception as e:
                print(f"âŒ éªŒè¯æ‰¹æ¬¡å‡ºé”™: {e}")

    # è®¡ç®—å„ç§è¯„ä¼°æŒ‡æ ‡
    # F1åˆ†æ•°
    f1_weighted = f1_score(trues, preds, average='weighted')
    f1_macro = f1_score(trues, preds, average='macro')
    f1_micro = f1_score(trues, preds, average='micro')
    
    # ç²¾ç¡®ç‡å’Œå¬å›ç‡
    precision_per_class = precision_score(trues, preds, average=None, labels=range(num_classes))
    recall_per_class = recall_score(trues, preds, average=None, labels=range(num_classes))
    precision_macro = precision_score(trues, preds, average='macro')
    recall_macro = recall_score(trues, preds, average='macro')
    precision_micro = precision_score(trues, preds, average='micro')
    recall_micro = recall_score(trues, preds, average='micro')
    
    # AUC (éœ€è¦å¤„ç†å¤šåˆ†ç±»)
    trues_bin = label_binarize(trues, classes=range(num_classes))
    if num_classes == 2:
        auc_roc = roc_auc_score(trues_bin, np.array(probs)[:, 1])
    else:
        try:
            auc_roc = roc_auc_score(trues_bin, probs, multi_class='ovr')
        except ValueError:
            auc_roc = 0.0  # å½“åªæœ‰ä¸€ä¸ªç±»åˆ«çš„æ—¶å€™ä¼šå‡ºç°é”™è¯¯
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(trues, preds)
    
    metrics = {
        'loss': total_loss / len(dataloader),
        'f1_weighted': f1_weighted,
        'f1_macro': f1_macro,
        'f1_micro': f1_micro,
        'precision_per_class': precision_per_class,
        'recall_per_class': recall_per_class,
        'precision_macro': precision_macro,
        'recall_macro': recall_macro,
        'precision_micro': precision_micro,
        'recall_micro': recall_micro,
        'auc_roc': auc_roc,
        'confusion_matrix': cm,
        'predictions': preds,
        'targets': trues,
        'probabilities': probs
    }
    
    # æœ€åæ¸…ç†
    torch.cuda.empty_cache()
    
    return metrics

def plot_confusion_matrix(cm, class_names, title='Confusion Matrix'):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µå¹¶è¿”å›å›¾åƒ"""
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names)
    plt.title(title)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.tight_layout()
    return plt.gcf()

def main_enhanced(file_path, use_enhanced_model=True, use_enhanced_features=True, use_coral=False, use_cascaded=False):
    """
    å¢å¼ºç‰ˆä¸»è®­ç»ƒå‡½æ•°
    
    Args:
        file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        use_enhanced_model: æ˜¯å¦ä½¿ç”¨å¢å¼ºæ¨¡å‹
        use_enhanced_features: æ˜¯å¦ä½¿ç”¨å¢å¼ºæ—¶é—´ç‰¹å¾
        use_coral: æ˜¯å¦ä½¿ç”¨CORALæ–¹æ³•
        use_cascaded: æ˜¯å¦ä½¿ç”¨çº§è”åˆ†ç±»å™¨
    """
    
    model_suffix = "enhanced" if use_enhanced_model else "standard"
    feature_suffix = "enhanced_features" if use_enhanced_features else "basic_features"
    coral_suffix = "_coral" if use_coral else ""
    cascaded_suffix = "_cascaded" if use_cascaded else ""
    
    # åˆå§‹åŒ–SwanLab
    swanlab.init(
        project="suicide-risk-enhanced",
        run_name=f"deberta_{model_suffix}_{feature_suffix}{coral_suffix}{cascaded_suffix}_5fold",
        config={
            "model_name": MODEL_NAME,
            "batch_size": 2, 
            "learning_rate": 2e-6,
            "max_len": 256,  
            "max_posts": 5,
            "hidden_dim": 512,
            "epochs": 15,
            "n_folds": 5,
            "weights": [1, 1.0, 1.0, 4.0],
            "use_enhanced_model": use_enhanced_model,
            "use_enhanced_features": use_enhanced_features,
            "use_debiased_attention": True,
            "use_coral": use_coral,
            "use_cascaded": use_cascaded,
            "time_feature_dim": 12 if use_enhanced_features else 8,
            "pattern_feature_dim": 6 if use_enhanced_features else 0
        }
    )
    
    config = swanlab.config
    
    # è¯»å–æ•°æ®
    df = pd.read_pickle(file_path)
    print(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆï¼Œæ€»æ ·æœ¬æ•°: {len(df)}")
    
    # æå–æ—¶é—´ç‰¹å¾
    print("ğŸ• æå–æ—¶é—´ç‰¹å¾ä¸­...")
    if use_enhanced_features:
        time_features = extract_enhanced_time_features(df)  # 12ç»´å¢å¼ºç‰¹å¾
        pattern_features = extract_temporal_patterns(df)    # 6ç»´æ—¶é—´æ¨¡å¼ç‰¹å¾
        print(f"âœ… å¢å¼ºæ—¶é—´ç‰¹å¾æå–å®Œæˆï¼ŒåŸºç¡€ç‰¹å¾: {time_features.shape}, æ¨¡å¼ç‰¹å¾: {pattern_features.shape}")
    else:
        # ä½¿ç”¨åŸç‰ˆç‰¹å¾æå–ï¼ˆä¿æŒå…¼å®¹ï¼‰
        from feature_utils import extract_time_features
        time_features = extract_time_features(df)  # 8ç»´åŸºç¡€ç‰¹å¾
        pattern_features = None
        print(f"âœ… åŸºç¡€æ—¶é—´ç‰¹å¾æå–å®Œæˆï¼Œç‰¹å¾ç»´åº¦: {time_features.shape}")
    
    # ç”Ÿæˆç”¨æˆ·çº§åˆ†å±‚äº¤å‰éªŒè¯æŠ˜
    folds = prepare_user_stratified_folds(df, n_splits=config.n_folds)
    
    all_fold_f1s = []
    
    for fold_idx, (train_df, val_df) in enumerate(folds, 1):
        print(f"\nğŸ“‚ Fold {fold_idx}/{len(folds)} - è®­ç»ƒæ ·æœ¬: {len(train_df)}, éªŒè¯æ ·æœ¬: {len(val_df)}")
        
        # æå–å¯¹åº”æŠ˜çš„æ—¶é—´ç‰¹å¾
        train_indices = train_df.index.tolist()
        val_indices = val_df.index.tolist()
        train_time_feats = time_features[train_indices]
        val_time_feats = time_features[val_indices]
        
        if use_enhanced_features and pattern_features is not None:
            train_pattern_feats = pattern_features[train_indices]
            val_pattern_feats = pattern_features[val_indices]
        else:
            train_pattern_feats = val_pattern_feats = None
        
        # åˆå§‹åŒ–tokenizerå’Œæ•°æ®é›†
        tokenizer = DebertaV2Tokenizer.from_pretrained(MODEL_NAME)
        
        if use_enhanced_features:
            # ä½¿ç”¨å¢å¼ºæ•°æ®é›†
            train_dataset = EnhancedSuicideDataset(
                train_df, tokenizer, 
                max_len=config.max_len, 
                max_posts=config.max_posts,
                time_features=train_time_feats,
                pattern_features=train_pattern_feats
            )
            val_dataset = EnhancedSuicideDataset(
                val_df, tokenizer, 
                max_len=config.max_len, 
                max_posts=config.max_posts,
                time_features=val_time_feats,
                pattern_features=val_pattern_feats
            )
        else:
            # ä½¿ç”¨åŸç‰ˆæ•°æ®é›†
            train_dataset = SuicideDataset(
                train_df, tokenizer, 
                max_len=config.max_len, 
                max_posts=config.max_posts,
                time_features=train_time_feats
            )
            val_dataset = SuicideDataset(
                val_df, tokenizer, 
                max_len=config.max_len, 
                max_posts=config.max_posts,
                time_features=val_time_feats
            )

        train_loader = DataLoader(
            train_dataset, 
            batch_size=config.batch_size, 
            shuffle=True, 
            num_workers=4, 
            pin_memory=True
        )
        val_loader = DataLoader(val_dataset, batch_size=config.batch_size)

        # åˆå§‹åŒ–æ¨¡å‹
        if use_coral and use_enhanced_model:
            # ä½¿ç”¨CORALå¢å¼ºæ¨¡å‹
            model = CoralEnhancedSuicideRiskClassifier(
                model_name=config.model_name, 
                hidden_dim=config.hidden_dim,
                time_feature_dim=config.time_feature_dim,
                pattern_feature_dim=config.pattern_feature_dim,
                use_debiased_attention=config.use_debiased_attention
            ).to(device)
        elif use_cascaded and use_enhanced_model:
            print("ä½¿ç”¨çº§è”å¢å¼ºæ¨¡å‹")
            # ä½¿ç”¨çº§è”å¢å¼ºæ¨¡å‹
            model = CascadedSuicideRiskClassifier(
                model_name=config.model_name, 
                hidden_dim=config.hidden_dim,
                time_feature_dim=config.time_feature_dim,
                pattern_feature_dim=config.pattern_feature_dim,
                use_debiased_attention=config.use_debiased_attention
            ).to(device)
        elif use_enhanced_model:
            # ä½¿ç”¨æ™®é€šå¢å¼ºæ¨¡å‹
            model = EnhancedSuicideRiskClassifier(
                model_name=config.model_name, 
                hidden_dim=config.hidden_dim,
                time_feature_dim=config.time_feature_dim,
                pattern_feature_dim=config.pattern_feature_dim,
                use_debiased_attention=config.use_debiased_attention
            ).to(device)
        else:
            # ä½¿ç”¨åŸç‰ˆæ¨¡å‹
            model = SuicideRiskClassifier(
                model_name=config.model_name, 
                hidden_dim=config.hidden_dim,
                time_feature_dim=config.time_feature_dim
            ).to(device)

        # æŸå¤±å‡½æ•°
        weights = torch.tensor(config.weights, dtype=torch.float).to(device)
        if use_coral:
            criterion = CoralLoss(weight=weights)
        elif use_cascaded:
            criterion = CascadedLoss(weights=None)  # çº§è”åˆ†ç±»å™¨æœ‰è‡ªå·±çš„æŸå¤±è®¡ç®—æ–¹å¼
        else:
            criterion = nn.CrossEntropyLoss(weight=weights)
        
        # ä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡forä¸åŒçš„æ¨¡å—
        if use_enhanced_model:
            bert_params = list(model.bert.parameters())
            other_params = [p for p in model.parameters() if not any(p is bp for bp in bert_params)]
            
            optimizer = torch.optim.AdamW([
                {'params': bert_params, 'lr': config.learning_rate},
                {'params': other_params, 'lr': config.learning_rate * 2}  # æ–°å¢å±‚ä½¿ç”¨æ›´é«˜å­¦ä¹ ç‡
            ])
        else:
            optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        num_training_steps = len(train_loader) * config.epochs
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=int(0.1 * num_training_steps),
            num_training_steps=num_training_steps
        )

        # è®­ç»ƒå‚æ•°
        best_f1 = 0.45
        patience = 10
        patience_counter = 0
        
        # å­˜å‚¨æ‰€æœ‰æŠ˜çš„è¯„ä¼°æŒ‡æ ‡
        all_fold_metrics = {
            'f1_weighted': [],
            'f1_macro': [],
            'f1_micro': [],
            'precision_macro': [],
            'recall_macro': [],
            'precision_micro': [],
            'recall_micro': [],
            'auc_roc': []
        }

        # å¼€å§‹è®­ç»ƒ
        for epoch in range(config.epochs):
            print(f"\nğŸš€ [Fold {fold_idx}] Epoch {epoch+1}/{config.epochs}")
            
            train_loss = train_epoch(
                model, train_loader, optimizer, criterion, scheduler, 
                use_enhanced=use_enhanced_features, use_coral=use_coral, use_cascaded=use_cascaded
            )
            
            # ä½¿ç”¨æ–°çš„è¯„ä¼°å‡½æ•°
            val_metrics = eval_epoch(
                model, criterion, val_loader, 
                use_enhanced=use_enhanced_features, use_coral=use_coral, use_cascaded=use_cascaded
            )
            
            val_loss = val_metrics['loss']
            val_f1_weighted = val_metrics['f1_weighted']
            val_f1_macro = val_metrics['f1_macro']
            val_f1_micro = val_metrics['f1_micro']
            val_precision_macro = val_metrics['precision_macro']
            val_recall_macro = val_metrics['recall_macro']
            val_precision_micro = val_metrics['precision_micro']
            val_recall_micro = val_metrics['recall_micro']
            val_auc_roc = val_metrics['auc_roc']
            val_precision_per_class = val_metrics['precision_per_class']
            val_recall_per_class = val_metrics['recall_per_class']
            cm = val_metrics['confusion_matrix']

            # è®°å½•æ—¥å¿—
            swanlab.log({
                f"fold_{fold_idx}/epoch": epoch + 1,
                f"fold_{fold_idx}/train_loss": train_loss,
                f"fold_{fold_idx}/val_loss": val_loss,
                f"fold_{fold_idx}/val_f1_weighted": val_f1_weighted,
                f"fold_{fold_idx}/val_f1_macro": val_f1_macro,
                f"fold_{fold_idx}/val_f1_micro": val_f1_micro,
                f"fold_{fold_idx}/val_precision_macro": val_precision_macro,
                f"fold_{fold_idx}/val_recall_macro": val_recall_macro,
                f"fold_{fold_idx}/val_precision_micro": val_precision_micro,
                f"fold_{fold_idx}/val_recall_micro": val_recall_micro,
                f"fold_{fold_idx}/val_auc_roc": val_auc_roc,
                f"fold_{fold_idx}/learning_rate": scheduler.get_last_lr()[0]
            })

            # è®°å½•æ¯ä¸ªç±»åˆ«çš„ç²¾ç¡®ç‡å’Œå¬å›ç‡
            class_names = ['Risk_0', 'Risk_1', 'Risk_2', 'Risk_3']
            for i, class_name in enumerate(class_names):
                swanlab.log({
                    f"fold_{fold_idx}/precision_{class_name}": val_precision_per_class[i],
                    f"fold_{fold_idx}/recall_{class_name}": val_recall_per_class[i]
                })

            print(f"[Fold {fold_idx}] Epoch {epoch+1} - Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")
            print(f"  F1 Scores - Weighted: {val_f1_weighted:.4f}, Macro: {val_f1_macro:.4f}, Micro: {val_f1_micro:.4f}")
            print(f"  Precision (Macro): {val_precision_macro:.4f}, Recall (Macro): {val_recall_macro:.4f}")
            print(f"  Precision (Micro): {val_precision_micro:.4f}, Recall (Micro): {val_recall_micro:.4f}")
            print(f"  AUC-ROC: {val_auc_roc:.4f}")
            
            # æ‰“å°æ¯ä¸ªç±»åˆ«çš„ç²¾ç¡®ç‡å’Œå¬å›ç‡
            print("  Per-class Metrics:")
            for i, class_name in enumerate(class_names):
                print(f"    {class_name} - Precision: {val_precision_per_class[i]:.4f}, Recall: {val_recall_per_class[i]:.4f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹ (åŸºäºweighted F1)
            if val_f1_weighted > best_f1:
                patience_counter = 0
                best_f1 = val_f1_weighted
                model_name = f'enhanced_fold{fold_idx}_f1{val_f1_weighted:.4f}_best.pt' if use_enhanced_model else f'standard_fold{fold_idx}_f1{val_f1_weighted:.4f}_best.pt'
                if use_coral:
                    model_name = f'coral_{model_name}'
                elif use_cascaded:
                    model_name = f'cascaded_{model_name}'
                torch.save(
                    model.state_dict(), 
                    os.path.join(save_path, model_name)
                )
                print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: F1 = {val_f1_weighted:.4f}")
                
                # ç”Ÿæˆå¹¶è®°å½•æ··æ·†çŸ©é˜µå›¾åƒ
                plt.clf()  # æ¸…é™¤ä¹‹å‰çš„å›¾åƒ
                cm_fig = plot_confusion_matrix(cm, class_names, f'Confusion Matrix - Fold {fold_idx}, Epoch {epoch+1}')
                swanlab.log({f"fold_{fold_idx}/confusion_matrix": swanlab.Image(cm_fig)})
                plt.close(cm_fig)  # å…³é—­å›¾åƒä»¥é‡Šæ”¾å†…å­˜
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f"â›” Fold {fold_idx} æ—©åœï¼Œæœ€ä½³F1: {best_f1:.4f}")
                    break
        
        # åœ¨æ¯ä¸ªfoldç»“æŸåï¼Œè®°å½•æœ€ç»ˆçš„è¯„ä¼°æŒ‡æ ‡
        all_fold_metrics['f1_weighted'].append(val_f1_weighted)
        all_fold_metrics['f1_macro'].append(val_f1_macro)
        all_fold_metrics['f1_micro'].append(val_f1_micro)
        all_fold_metrics['precision_macro'].append(val_precision_macro)
        all_fold_metrics['recall_macro'].append(val_recall_macro)
        all_fold_metrics['precision_micro'].append(val_precision_micro)
        all_fold_metrics['recall_micro'].append(val_recall_micro)
        all_fold_metrics['auc_roc'].append(val_auc_roc)
        
        print(f"âœ… Fold {fold_idx} å®Œæˆï¼Œæœ€ä½³F1-weighted: {best_f1:.4f}")
    
    # è®¡ç®—å¹³å‡æ€§èƒ½
    mean_metrics = {}
    std_metrics = {}
    for metric_name, values in all_fold_metrics.items():
        mean_metrics[metric_name] = np.mean(values)
        std_metrics[metric_name] = np.std(values)
    
    # è®°å½•æ€»ä½“ç»Ÿè®¡ä¿¡æ¯
    swanlab.log({
        "overall/mean_f1_weighted": mean_metrics['f1_weighted'],
        "overall/std_f1_weighted": std_metrics['f1_weighted'],
        "overall/mean_f1_macro": mean_metrics['f1_macro'],
        "overall/std_f1_macro": std_metrics['f1_macro'],
        "overall/mean_f1_micro": mean_metrics['f1_micro'],
        "overall/std_f1_micro": std_metrics['f1_micro'],
        "overall/mean_precision_macro": mean_metrics['precision_macro'],
        "overall/std_precision_macro": std_metrics['precision_macro'],
        "overall/mean_recall_macro": mean_metrics['recall_macro'],
        "overall/std_recall_macro": std_metrics['recall_macro'],
        "overall/mean_precision_micro": mean_metrics['precision_micro'],
        "overall/std_precision_micro": std_metrics['precision_micro'],
        "overall/mean_recall_micro": mean_metrics['recall_micro'],
        "overall/std_recall_micro": std_metrics['recall_micro'],
        "overall/mean_auc_roc": mean_metrics['auc_roc'],
        "overall/std_auc_roc": std_metrics['auc_roc']
    })
    
    print(f"\nğŸ“Š 5æŠ˜äº¤å‰éªŒè¯ç»“æœ:")
    print(f"æ¨¡å‹ç±»å‹: {'å¢å¼ºæ¨¡å‹' if use_enhanced_model else 'æ ‡å‡†æ¨¡å‹'}")
    print(f"ç‰¹å¾ç±»å‹: {'å¢å¼ºç‰¹å¾' if use_enhanced_features else 'åŸºç¡€ç‰¹å¾'}")
    print(f"CORALæ–¹æ³•: {'æ˜¯' if use_coral else 'å¦'}")
    print(f"çº§è”åˆ†ç±»å™¨: {'æ˜¯' if use_cascaded else 'å¦'}")
    print(f"å„æŠ˜F1-weightedåˆ†æ•°: {[f'{f1:.4f}' for f1 in all_fold_metrics['f1_weighted']]}")
    print(f"å¹³å‡F1-weighted: {mean_metrics['f1_weighted']:.4f} Â± {std_metrics['f1_weighted']:.4f}")
    print(f"å¹³å‡F1-macro: {mean_metrics['f1_macro']:.4f} Â± {std_metrics['f1_macro']:.4f}")
    print(f"å¹³å‡F1-micro: {mean_metrics['f1_micro']:.4f} Â± {std_metrics['f1_micro']:.4f}")
    print(f"å¹³å‡Precision-macro: {mean_metrics['precision_macro']:.4f} Â± {std_metrics['precision_macro']:.4f}")
    print(f"å¹³å‡Recall-macro: {mean_metrics['recall_macro']:.4f} Â± {std_metrics['recall_macro']:.4f}")
    print(f"å¹³å‡Precision-micro: {mean_metrics['precision_micro']:.4f} Â± {std_metrics['precision_micro']:.4f}")
    print(f"å¹³å‡Recall-micro: {mean_metrics['recall_micro']:.4f} Â± {std_metrics['recall_micro']:.4f}")
    print(f"å¹³å‡AUC-ROC: {mean_metrics['auc_roc']:.4f} Â± {std_metrics['auc_roc']:.4f}")
    
    swanlab.finish()

if __name__ == '__main__':
    
    # ç¡®ä¿ä¿å­˜è·¯å¾„å­˜åœ¨
    os.makedirs(save_path, exist_ok=True)
    
    # è¿è¡Œä¸åŒé…ç½®çš„å®éªŒ
 #   print("ğŸš€ å®éªŒ1: å¢å¼ºæ¨¡å‹ + å¢å¼ºç‰¹å¾")
 #   main_enhanced(file_path, use_enhanced_model=True, use_enhanced_features=True, use_coral=False, use_cascaded=False)
    
#    print("ğŸš€ å®éªŒ2: å¢å¼ºæ¨¡å‹ + å¢å¼ºç‰¹å¾ + CORAL")
#    main_enhanced(file_path, use_enhanced_model=True, use_enhanced_features=True, use_coral=True, use_cascaded=False)
    
    print("ğŸš€ å®éªŒ3: å¢å¼ºæ¨¡å‹ + å¢å¼ºç‰¹å¾ + çº§è”åˆ†ç±»å™¨")
    main_enhanced(file_path, use_enhanced_model=True, use_enhanced_features=True, use_coral=False, use_cascaded=True)
    
    # print("\nğŸš€ å®éªŒ4: æ ‡å‡†æ¨¡å‹ + åŸºç¡€ç‰¹å¾")
    # main_enhanced(file_path, use_enhanced_model=False, use_enhanced_features=False)
    
    # print("\nğŸš€ å®éªŒ5: å¢å¼ºæ¨¡å‹ + åŸºç¡€ç‰¹å¾")
    # main_enhanced(file_path, use_enhanced_model=True, use_enhanced_features=False)


